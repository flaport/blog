{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning from the ground up | part 2: tic-tac-toe revisited for deep Q learning\n",
    "Previously, we saw that reinforcement learning worked quite well on tic-tac-toe. However, there's something unsatisfying about working with a Q-table storing all the possible states of the game. It feels like the Agent simply memorizes each state of the game and acts according to some memorized rules obtained by its huge amount of experience (remember that the Agent played 10,000,000 games during training). In this second part of the reinforcement learning series, we'll swap out the Q table for a *neural network*.\n",
    "\n",
    "<img src=\"static/img/reinforcement-learning/tictactoe.svg\" width=200>\n",
    "\n",
    "~~**The AI created in this series can now be challenged [here](#)!**~~ [offline at the moment]\n",
    "\n",
    "We'll use PyTorch to create and optimize the neural network, as its excellent compatibility with numpy allows us to swap out the Qtable for a deep Q network with minimal effort. Also, it's just the best deep-learning framework for python - no competition really."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "* [part 1](reinforcement-learning-part-1.ipynb): We create the game environment and a simple unbeatable AI based on *traditional* Q-learning ðŸ¤–.\n",
    "* part 2 **(this post)**: We modify our AI to utilize a neural network: *deep* Q-learning ðŸ‘¾."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!\n",
    "# standard library:\n",
    "import json  # to store learned state\n",
    "\n",
    "# 3rd party:\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# settings\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-model\n",
    "The main difference with the [previous part](20190527_reinforcement-learning-on-tictactoe-part-1) of the series is that we'll swap out the Q-table by a *deep Q neural network model* of 3 layers deep. OK, that's not very deep, but we'll stick with the buzzwords.\n",
    "\n",
    "The goal of our `QModel` is to accurately *approximate* the values in the `QTable`. We could thus train the neural network on the values obtained in the previous part, but obviously that would defeat the purpose of using a neural network in the first place (as it requires you to find the underlying Qtable first).\n",
    "\n",
    "We want the neural network to act as a *proxy* for the table. It also has to be able to approximate the QTable in cases it is completely unfeasible to find the underlying QTable.\n",
    "\n",
    "The `QModel` defined below expects the full game state (2D game state and the turn). The 2D state will be flattened and concatenated with the turn index. This concatenated state of length 10 with three kinds of states per cell (`1`, `2` or `0`) is then embedded in a three dimensional embedding space. Next, the embedded game state is sent through 3 neural network layers with ReLU activation.\n",
    "\n",
    "Note that in this case, the turn index *is* used to make a prediction. This is in contrast with the Q-table case, where just the 2D state was used. The reason for this is that in this case we work with a neural network for which the predicted Q-values are note completely independent for each state given. We thus want to give as much information to the network as possible and let the network itself figure out which information it uses.\n",
    "\n",
    "Apart from the network definition, we also created some `save` and `load` methods to save the weights of the network as `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(3, 3)\n",
    "        self.layer1 = torch.nn.Linear(30, 300)\n",
    "        self.layer2 = torch.nn.Linear(300, 300)\n",
    "        self.layer3 = torch.nn.Linear(300, 9)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, states2d, turns):\n",
    "        if not torch.is_tensor(states2d):\n",
    "            states2d = torch.from_numpy(states2d)\n",
    "        if not torch.is_tensor(turns):\n",
    "            turns = torch.from_numpy(turns)\n",
    "        assert states2d.dim() == 3 # batch dimension required\n",
    "        assert turns.dim() == 1 # only dim = batch dim\n",
    "        x = torch.cat([states2d.flatten(1), turns[:,None]], 1)\n",
    "        x = self.relu(self.embedding(x)).flatten(1)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def _serialize_tensor(self, tensor):\n",
    "        if tensor.dim() == 0:\n",
    "            return float(tensor)\n",
    "        return [self._serialize_tensor(t) for t in tensor]\n",
    "\n",
    "    def _deserialize_tensor(self, tensor):\n",
    "        return torch.tensor(tensor, dtype=torch.get_default_dtype())\n",
    "\n",
    "    def save(self, filename):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            filename += \".json\"\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(\n",
    "                {k: self._serialize_tensor(t) for k, t in self.state_dict().items()},\n",
    "                file,\n",
    "            )\n",
    "\n",
    "    def load(self, filename):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            filename += \".json\"\n",
    "        with open(filename, \"r\") as file:\n",
    "            self.load_state_dict(\n",
    "                {k: self._deserialize_tensor(t) for k, t in json.load(file).items()}\n",
    "            )\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game\n",
    "In the [previous part](reinforcement-learning-part-1.ipynb), we assumed the Agent learned after every turn played. This form of *online* learning worked well for a Q-table, as each state has an independent row of qvalues associated with it. \n",
    "\n",
    "Unfortunaty, when using a neural network, predicted Q values will not be completely independent for each different state. This is a good thing, as it allows the neural network to generalize and invoke similar behavior for similar states.\n",
    "\n",
    "However, it also makes the training process less stable. To be able to stabilize the training, we'll have to batch several *transitions* - (`state`, `action`, `next_state`, `reward`) sequences - between states together.\n",
    "\n",
    "Then, after a few games are played, all transitions are batched together and the Agent can learn from each transition in the batch simultaneously.\n",
    "\n",
    "Let's see how we can implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\" Tic-Tac-Toe Game \"\"\"\n",
    "\n",
    "    def __init__(self, player1, player2):\n",
    "        \"\"\" The Tic-Tac-Toe game takes two players and pitches them against each other. \"\"\"\n",
    "        # pitch players against each other\n",
    "        self.players = {1: player1, 2: player2}\n",
    "\n",
    "        # reward for each outcome of the game (tie, player1 wins, player2 wins)\n",
    "        self._reward = {0: 0, 1: 1, 2: -1}\n",
    "\n",
    "    def play(self, num_games=1, visualize=False):\n",
    "        \"\"\" play several full games \"\"\"\n",
    "\n",
    "        transitions = []\n",
    "        for _ in range(num_games):\n",
    "            turn = 1\n",
    "            state2d = np.zeros((3,3), dtype=np.int64)\n",
    "            state = (state2d, turn) # full state of the game\n",
    "            for i in range(9):\n",
    "                current_player = self.players[turn]\n",
    "                action = current_player.get_action(state)\n",
    "                next_state, reward = self.play_turn(state, action)\n",
    "                transitions.append(\n",
    "                    (state, action, next_state, reward)\n",
    "                )\n",
    "                if visualize:\n",
    "                    self.visualize_state(next_state, turn)\n",
    "                    \n",
    "                (state2d, turn) = state = next_state\n",
    "                \n",
    "                if turn == 0:\n",
    "                    break\n",
    "\n",
    "        return transitions\n",
    "    \n",
    "    def play_turn(self, state, action):\n",
    "        \"\"\" execute a specific move chosen by the current player and \n",
    "        check if it's a winning/losing move. \"\"\"\n",
    "        # retrieve states\n",
    "        state2d, turn = state\n",
    "        next_state2d = state2d.copy()\n",
    "        next_turn = turn % 2 + 1\n",
    "\n",
    "        # transform action in two indices\n",
    "        ax, ay = action // 3, action % 3\n",
    "\n",
    "        # check if board is already occupied at location\n",
    "        if state2d[ax, ay] != 0:  # invalid move\n",
    "            next_state2d.fill(0)\n",
    "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
    "            return next_state, self._reward[next_turn]  # next player wins\n",
    "\n",
    "        # apply action\n",
    "        next_state2d[ax, ay] = turn\n",
    "\n",
    "        # check if the action resulted in a winner\n",
    "        mask = next_state2d == turn\n",
    "        if (\n",
    "            (mask[0, 0] and mask[1, 1] and mask[2, 2])\n",
    "            or (mask[0, 2] and mask[1, 1] and mask[2, 0])\n",
    "            or (mask[0, 0] and mask[0, 1] and mask[0, 2])\n",
    "            or (mask[1, 0] and mask[1, 1] and mask[1, 2])\n",
    "            or (mask[2, 0] and mask[2, 1] and mask[2, 2])\n",
    "            or (mask[0, 0] and mask[1, 0] and mask[2, 0])\n",
    "            or (mask[0, 1] and mask[1, 1] and mask[2, 1])\n",
    "            or (mask[0, 2] and mask[1, 2] and mask[2, 2])\n",
    "        ):\n",
    "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
    "            return next_state, self._reward[turn]  # current player wins\n",
    "\n",
    "        # if the playing board is full, but no winner found: tie\n",
    "        if (next_state2d != 0).all():  # final tie.\n",
    "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
    "            return next_state, self._reward[0]  # no winner\n",
    "\n",
    "        # if no move has resulted in a winner: next player's turn.\n",
    "        next_state = (next_state2d, next_turn)\n",
    "        return next_state, self._reward[0]  # no winner yet\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_state(next_state, turn):\n",
    "        \"\"\" show the resulting game state after a player's turn \"\"\"\n",
    "        next_state2d, next_turn = next_state\n",
    "        print(f\"player {turn}'s turn:\")\n",
    "        if (next_state2d == 0).all() and turn == 0:\n",
    "            print(\"[invalid state]\\n\\n\")\n",
    "        else:\n",
    "            print(\n",
    "                str(next_state2d)\n",
    "                .replace(\"[[\", \"\")\n",
    "                .replace(\" [\", \"\")\n",
    "                .replace(\"]]\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                .replace(\"0\", \".\")\n",
    "                .replace(\"1\", \"O\")\n",
    "                .replace(\"2\", \"X\")\n",
    "                + \"\\n\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep QAgent\n",
    "\n",
    "<img src=\"static/img/reinforcement-learning/robot.png\" width=200>\n",
    "\n",
    "Finally, we have to adapt the Agent to\n",
    "\n",
    "1. Use the `QModel` in stead of the `QTable`\n",
    "2. Learn from multiple transitions at once instead of a single one.\n",
    "\n",
    "One of the main observations used to be able to learn from a batch of transitions at once is the observation that:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\rm argmin(\\texttt{values}) = argmax(-\\texttt{values})\n",
    "\\end{align*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align*}\n",
    "    \\rm min(\\texttt{values}) = -max(-\\texttt{values})\n",
    "\\end{align*}\n",
    "\n",
    "This allows us to unify the policies for player1 and player2 in a single policy which depends on the parity `s` of your turn (`+1` for player 1, `-1` for player 2):\n",
    "\n",
    "\\begin{align*}\n",
    "    \\texttt{action} &= \\pi(\\texttt{state}) = \\text{argmax}_{action} \\left( s \\cdot Q(\\texttt{state},~\\texttt{action}) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "This same trick can also be used to calculate the error Î´ between the expected qvalues and the discounted future qvalues:\n",
    "\\begin{align*}\n",
    "    \\delta &= Q(\\texttt{state},~\\texttt{action}) - (\\texttt{reward} + s \\cdot \\texttt{discount_factor} \\cdot \\text{max}_{\\texttt{action}} \\left( s \\cdot Q(\\texttt{next-state},~\\texttt{action})) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Unfortunately, this error can not be used explicitly anymore for updating the qtable. In stead, we use PyTorch's Adam optimizer to minimize the *Huber Loss*, which is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{split}\\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "\\end{cases}\\end{split}\n",
    "\\end{align*}\n",
    "The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large. This makes it more robust to outliers when the estimates of Q are very noisy - which is very often the case with reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" The Agent plays the game by playing a move corresponding to the optimal Q-value \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, qmodel=None, epsilon=0.2, learning_rate=0.01, discount_factor=0.9\n",
    "    ):\n",
    "        \"\"\" A new Agent can be given some optional parameters to tune how fast it learns\n",
    "        \n",
    "        Args:\n",
    "            qmodel: QModel=None: the initial Q-table to start with. \n",
    "            epsilon: float=0.2: the chance the Agent will explore a random move\n",
    "                               (in stead of choosing the optimal choice according to the Q table)\n",
    "            learning_rate: float=0.3: the rate at which the Agent learns from its games\n",
    "            discount_factor: float=0.9: the rate at which the final reward gets discounted\n",
    "                                        for when rating previous moves.\n",
    "        \"\"\"\n",
    "        self.qmodel = QModel() if qmodel is None else qmodel\n",
    "\n",
    "        # the speed at which the Qvalues get updated\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # pytorch optimizer to update the weights of the QModel\n",
    "        self._optimizer = torch.optim.Adam(self.qmodel.parameters(), lr=learning_rate)\n",
    "\n",
    "        # the discount factor of future rewards\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # the chance of executing a random action\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def random_action(self):\n",
    "        \"\"\" get a random action chosen from the allowed actions to take \"\"\"\n",
    "        return int(np.random.randint(0, 9, 1, dtype=np.int64))\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" get the best values according to the current Q table \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state2d, turn = state\n",
    "            sign = np.float64(1 - 2 * (turn - 1))\n",
    "            turns = torch.tensor(turn, dtype=torch.int64)[None]  # batch dim required\n",
    "            states2d = torch.tensor(state2d, dtype=torch.int64)[None]\n",
    "            qvalues = self.qmodel(states2d, turns)[0]\n",
    "        return np.argmax(sign * qvalues)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\" perform an action according to the state on the game board \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Choose action (random with chance of epsilon; best action otherwise.)\n",
    "            action = self.random_action()\n",
    "        else:\n",
    "            # get qvalues for current state of the game\n",
    "            action = self.best_action(state)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, transitions):\n",
    "        \"\"\" learn from the current state and action taken. \"\"\"\n",
    "        states, actions, next_states, rewards = zip(*transitions)\n",
    "        states2d, turns = zip(*states)\n",
    "        next_states2d, next_turns = zip(*next_states)\n",
    "        turns = torch.tensor(turns, dtype=torch.int64)\n",
    "        next_turns = torch.tensor(next_turns, dtype=torch.int64)\n",
    "        states2d = torch.tensor(states2d, dtype=torch.int64)\n",
    "        next_states2d = torch.tensor(next_states2d, dtype=torch.int64)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            # get qvalues for current state of the game\n",
    "            mask = (next_turns > 0).float()  # wether the game is over or not\n",
    "            signs = (1 - 2 * (next_turns - 1)).float()\n",
    "            next_qvalues = self.qmodel(next_states2d, next_turns)\n",
    "            expected_qvalues_for_actions = rewards + mask * signs * (\n",
    "                self.discount_factor * torch.max(signs[:, None] * next_qvalues, 1)[0]\n",
    "            )\n",
    "\n",
    "        # update qvalues:\n",
    "        qvalues_for_actions = torch.gather(\n",
    "            self.qmodel(states2d, turns), dim=1, index=actions[:, None]\n",
    "        ).view(-1)\n",
    "        loss = torch.nn.functional.smooth_l1_loss(\n",
    "            qvalues_for_actions, expected_qvalues_for_actions\n",
    "        )\n",
    "        self._optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "<img src=\"static/img/reinforcement-learning/train.jpg\" width=300>\n",
    "\n",
    "We let the agent now train in batches of 1,000 games, for 15,000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [2:18:48<00:00,  1.81it/s, loss=0.00184, min_loss=0.001]   \n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(1)\n",
    "total_number_of_games = 15_000_000 # 15_000_000\n",
    "number_of_games_per_batch = 1000\n",
    "\n",
    "player = Agent(epsilon=0.7, learning_rate=0.01) # 0.7 0.01\n",
    "game = TicTacToe(player, player)\n",
    "\n",
    "min_loss = np.inf\n",
    "range_ = tqdm.trange(total_number_of_games // number_of_games_per_batch)\n",
    "for i in range_:\n",
    "    transitions = game.play(num_games=number_of_games_per_batch)\n",
    "    np.random.shuffle(transitions)\n",
    "    loss = player.learn(transitions)\n",
    "\n",
    "    if loss < min_loss and loss < 0.01:\n",
    "        min_loss = loss\n",
    "\n",
    "    range_.set_postfix(loss=loss, min_loss=min_loss)\n",
    "\n",
    "player.qmodel.save(\"qmodel.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play another game!\n",
    "Let the trained agent play against itself one final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1's turn:\n",
      ". . .\n",
      ". O .\n",
      ". . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . .\n",
      ". O .\n",
      ". . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . .\n",
      "O O .\n",
      ". . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . .\n",
      "O O X\n",
      ". . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . .\n",
      "O O X\n",
      ". O .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X X .\n",
      "O O X\n",
      ". O .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X X O\n",
      "O O X\n",
      ". O .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X X O\n",
      "O O X\n",
      "X O .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X X O\n",
      "O O X\n",
      "X O O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player = Agent(epsilon=0.0)  # epsilon=0 -> no random guesses\n",
    "game = TicTacToe(player, player)\n",
    "player.qmodel.load(\"qmodel.json\")\n",
    "\n",
    "# play\n",
    "game.play(num_games=1, visualize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the game also ends in a tie, providing some evidence that the Agent is as good as the one we made in the previous post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-model for the trained Agent can be downloaded in json format [here](blob/qmodel.json) (availability not guarenteed)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
