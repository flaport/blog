{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning from the ground up | part 1: tic-tac-toe.\n",
    "As a first example to reinforcement learning, we'll make our computer learn by itself how to play tic-tac-toe. As one of the most simple 2 player games, tic-tac-toe is ideal to get started with reinforcement learning, while still being more interesting that learning to play a single player game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/img/reinforcement-learning/tictactoe.svg\" width=200>\n",
    "\n",
    "**The AI created in this series can now be challenged [here](reinforcement-learning-part-3.html)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "* part 1 **(this post)**: We create the game environment and a simple unbeatable AI based on *traditional* Q-learning ü§ñ.\n",
    "* [part 2](reinforcement-learning-part-2.ipynb): We modify our AI to utilize a neural network: *deep* Q-learning üëæ.\n",
    "* [part 3](reinforcement-learning-part-3.html): Have some fun and play against the Q-agent ü§ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!\n",
    "# standard library:\n",
    "import json  # to store learned state\n",
    "\n",
    "# 3rd party:\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# settings\n",
    "np.random.seed(1)  # set seed for reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game\n",
    "\n",
    "First we'll need to create an environment to play te game. We'll represent this environment by the `TicTacToe` class.\n",
    "\n",
    "A `TicTacToe` object stores the *2D state* of the game, which is represented as a 2D numpy array with 9 elements, such that the positions of the playing board are labeled as follows. Each of those labels represents a possible *action* an player can take.\n",
    "```python\n",
    "0   1   2\n",
    "3   4   5\n",
    "6   7   8\n",
    "```\n",
    "Furthermore, each position on the board can be in 3 differents states. First there is the empty state (a position in the board where nobody has placed a mark yet) denoted by a `0`. Then there is the `O`-state: the state where `player1` has placed a mark denoted by a `1`. Finally there is the `X`-state, denoted by a `2`: the `state` where `player2` has placed a mark.\n",
    "\n",
    "An example state of the board at a certain time in the game can for example look like this:\n",
    "```python\n",
    ".   X   O       0   2   1\n",
    ".   O   .  -->  0   1   0 \n",
    "X   O   X       2   1   2\n",
    "```\n",
    "\n",
    "In addition to these 9 numbers, the current player's *turn* will also be kept track of. The turn index can thus be either `1` or `2`, or `0` when the game is over. The *turn* together with the *2D state* make up the full *state* of the game. \n",
    "\n",
    "Apart from the state of the game, the `TicTacToe` object will also keep a reference to the two players playing the game, so it can ask for an action from either player whenever it's the specific `player`'s turn. A `player` is asked to choose an `action` (move) by its own `get_action` method.\n",
    "\n",
    "After the `action` is asked from a `player`, the `action` is validated and executed by the `play_turn` method of the `TicTacToe` instance. \n",
    "If the action is invalid (for example when choosing a square that is already taken), the player loses. If the `action` results in a line of three equal marks, the current `player` wins. This would result in a `+1` reward if `player1` wins and a `-1` reward if `player2` wins (they have opposite goals, so they receive opposite rewards; more on the reward system later). In all other cases no rewards will be given as no `player` has won (yet).\n",
    "\n",
    "Finally, the full `state`, `action` and resulting `next_state` and `reward` are given to the player's `learn` method, such that the player can update it's policy according to the reward received, which will make it learn from its mistakes.\n",
    "\n",
    "Finally, it is useful to have a way to visualize the game each turn. The game `state` can be visualized by setting the `visualize` flag to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\" Tic-Tac-Toe Game \"\"\"\n",
    "\n",
    "    def __init__(self, player1, player2):\n",
    "        \"\"\" The Tic-Tac-Toe game takes two players and pitches them against each other. \"\"\"\n",
    "        # pitch players against each other\n",
    "        self.players = {1: player1, 2: player2}\n",
    "\n",
    "        # reward for each outcome of the game (tie, player1 wins, player2 wins)\n",
    "        self._reward = {0: 0, 1: 1, 2: -1}\n",
    "\n",
    "    def play(self, visualize=False):\n",
    "        \"\"\" play a full game \"\"\"\n",
    "        turn = 1\n",
    "        state2d = np.zeros((3, 3), dtype=np.int64)\n",
    "        state = (state2d, turn)  # full state of the game\n",
    "        for i in range(9):\n",
    "            current_player = self.players[turn]\n",
    "            action = current_player.get_action(state)\n",
    "            next_state, reward = self.play_turn(state, action)\n",
    "            current_player.learn(state, action, next_state, reward)\n",
    "\n",
    "            if visualize:\n",
    "                self.visualize_state(next_state, turn)\n",
    "\n",
    "            (state2d, turn) = state = next_state\n",
    "\n",
    "            if turn == 0:\n",
    "                break\n",
    "\n",
    "    def play_turn(self, state, action):\n",
    "        \"\"\" execute a specific move chosen by the current player and \n",
    "        check if it's a winning/losing move. \"\"\"\n",
    "        # retrieve states\n",
    "        state2d, turn = state\n",
    "        next_state2d = state2d.copy()\n",
    "        next_turn = turn % 2 + 1\n",
    "\n",
    "        # transform action in two indices\n",
    "        ax, ay = action // 3, action % 3\n",
    "\n",
    "        # check if board is already occupied at location\n",
    "        if state2d[ax, ay] != 0:  # invalid move\n",
    "            next_state2d.fill(0)\n",
    "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
    "            return next_state, self._reward[next_turn]  # next player wins\n",
    "\n",
    "        # apply action\n",
    "        next_state2d[ax, ay] = turn\n",
    "\n",
    "        # check if the action resulted in a winner\n",
    "        mask = next_state2d == turn\n",
    "        if (\n",
    "            (mask[0, 0] and mask[1, 1] and mask[2, 2])\n",
    "            or (mask[0, 2] and mask[1, 1] and mask[2, 0])\n",
    "            or (mask[0, 0] and mask[0, 1] and mask[0, 2])\n",
    "            or (mask[1, 0] and mask[1, 1] and mask[1, 2])\n",
    "            or (mask[2, 0] and mask[2, 1] and mask[2, 2])\n",
    "            or (mask[0, 0] and mask[1, 0] and mask[2, 0])\n",
    "            or (mask[0, 1] and mask[1, 1] and mask[2, 1])\n",
    "            or (mask[0, 2] and mask[1, 2] and mask[2, 2])\n",
    "        ):\n",
    "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
    "            return next_state, self._reward[turn]  # current player wins\n",
    "\n",
    "        # if the playing board is full, but no winner found: tie\n",
    "        if (next_state2d != 0).all():  # final tie.\n",
    "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
    "            return next_state, self._reward[0]  # no winner\n",
    "\n",
    "        # if no move has resulted in a winner: next player's turn.\n",
    "        next_state = (next_state2d, next_turn)\n",
    "        return next_state, self._reward[0]  # no winner yet\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_state(next_state, turn):\n",
    "        \"\"\" show the resulting game state after a player's turn \"\"\"\n",
    "        next_state2d, next_turn = next_state\n",
    "        print(f\"player {turn}'s turn:\")\n",
    "        if (next_state2d == 0).all() and turn == 0:\n",
    "            print(\"[invalid state]\\n\\n\")\n",
    "        else:\n",
    "            print(\n",
    "                str(next_state2d)\n",
    "                .replace(\"[[\", \"\")\n",
    "                .replace(\" [\", \"\")\n",
    "                .replace(\"]]\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                .replace(\"0\", \".\")\n",
    "                .replace(\"1\", \"O\")\n",
    "                .replace(\"2\", \"X\")\n",
    "                + \"\\n\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Player\n",
    "\n",
    "<img src=\"static/img/reinforcement-learning/dice.svg\" width=200>\n",
    "\n",
    "Let's first define the most stupid player of them all: a random player.\n",
    "The following player will always return a random index corresponding to one of the empty positions in the board:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    \"\"\" The random player will return a random action from the set of allowed actions\"\"\"\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state2d, turn = state\n",
    "        possible_actions = np.where(state2d.ravel() == 0)[0]\n",
    "        action = np.random.choice(possible_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward):\n",
    "        pass  # a random player does not learn from its mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play a game!\n",
    "Let's pitch two of those random players against each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1's turn:\n",
      ". . .\n",
      ". . O\n",
      ". . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      ". . .\n",
      "X . O\n",
      ". . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      ". . .\n",
      "X . O\n",
      "O . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . .\n",
      "X . O\n",
      "O . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . O\n",
      "X . O\n",
      "O . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . O\n",
      "X . O\n",
      "O . X\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . O\n",
      "X O O\n",
      "O . X\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player1 = RandomPlayer()\n",
    "player2 = RandomPlayer()\n",
    "game = TicTacToe(player1, player2)\n",
    "game.play(visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviously, this is not very interesting. We need a smarter kind of player:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A smarter player\n",
    "\n",
    "But how do we define such a smarter `player`? We could probably just code a perfect AI ourselves by coding out some rules that the `player` has to take in each specific position of the board. And for tic-tac-toe, this would probably not be very difficult. However, we'd like a more general approach: enter `Q` learning.\n",
    "\n",
    "In general, `Q` learning works af follows: An `Agent` plays the game according to a certain policy `œÄ`. This policy `œÄ` is such that it chooses the `action` with the maximal `Q` value associated with it for a specific `state` of the game. This `Q` value can be found in a lookup table: the `Q` table, where a different `Q` value is associated with each `state` of the game and the possible `action`s one can take for this specific `state`. Mathematically, one can express this as follows:\n",
    "\\begin{align*}\n",
    "    \\texttt{action} &= \\pi(\\texttt{state}) = \\text{argmax}_{action} Q(\\texttt{state},~\\texttt{action})\n",
    "\\end{align*}\n",
    "Intuitively, each `Q` value can thus vaguely be linked to the probability of a certain `player` winning the game after playing a certain `action` when the game is in a certain `state`.\n",
    "\n",
    "However, we will pitch two Agents against each other. If we assume a higher `Q` value to be associated to a higher chance of `player1` winning, the policy for `player2` will have to be the opposite of the policy of `player1`. `player2`'s policy will thus try to choose the `action` with the minimal `Q` value associated to it. This way of pitching two players against each other while one tries to optimize the objective function and the other tries to minimize it is also known as the `minimax` algorithm.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\texttt{player1} &: \\texttt{action} = \\pi(\\texttt{state}) = \\text{argmax}_{action} Q(\\texttt{state},~\\texttt{action})\\\\\n",
    "    \\texttt{player2} &: \\texttt{action} = \\pi(\\texttt{state}) = \\text{argmin}_{action} Q(\\texttt{state},~\\texttt{action})\n",
    "\\end{align*}\n",
    "\n",
    "However, the above does not fully describe how the `Q` table is formed. We need to define a way to update the `Q` values after a certain decision has been made. For this, the Bellman equation can be used, which basically describes that the Q-value at the current state of the game should be equal to the reward received at this stage of the game plus a discounted Q-value at the next state of the game.\n",
    "\\begin{align*}\n",
    "    Q(\\texttt{state},~\\texttt{action}) = \\texttt{reward} + \\texttt{discount_factor} \\cdot Q(\\texttt{next-state},~\\texttt{next-action})\n",
    "\\end{align*}\n",
    "Which states that for a *perfect* `Q` table, the expected `Q` value of the current `state` will be equal the the `Q` value of the next `state` and next `action` discounted by a `discount_factor` plus a possible reward obtained by performing chosen the `action`. However, we start with an imperfect `Q` table, which means that at each timestep, there will be an error `Œ¥` between the left hand side and the right hand side of the Bellman equation:\n",
    "\\begin{align*}\n",
    "    \\delta &= Q(\\texttt{state},~\\texttt{action}) - (\\texttt{reward} + \\texttt{discount_factor} \\cdot Q(\\texttt{next-state},~\\texttt{next-action}))\n",
    "\\end{align*}\n",
    "`player1` has the policy to *maximize* `Q`, while `player2` will have the policy to *minimize* `Q` and vice versa, which yields for the error on the `Q` values for each of the players:\n",
    "\\begin{align*}\n",
    "    \\texttt{player1} &: \\delta = Q(\\texttt{state},~\\texttt{action}) - (\\texttt{reward} + \\texttt{discount_factor} \\cdot \\text{min}_{\\texttt{action}} Q(\\texttt{next-state},~\\texttt{action})) \\\\\n",
    "    \\texttt{player2} &: \\delta = Q(\\texttt{state},~\\texttt{action}) - (\\texttt{reward} + \\texttt{discount_factor} \\cdot \\text{max}_{\\texttt{action}} Q(\\texttt{next-state},~\\texttt{action}))\n",
    "\\end{align*}\n",
    "Note that the error on the Q-value for player 1 uses the policy of player 2 [`min`]. This is to be expected, as this policy appears in the discounted Q-value of the next state, which is determined by the other player.\n",
    "\n",
    "This error can then be used in conjunction with a `learning_rate` to update the `Q` values iteratively at every step of the game.\n",
    "\n",
    "Let's have a look how this can be implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q table\n",
    "\n",
    "As discussed above, the Q table is just a table (numpy array) that returns a Q value for each state of the game and for each action taken. A tic-tac-toe game consists of 9 squares, each of which can be in 3 different states, totalling 3^9 different states (in reality, there are less allowed states possible; 3^9 is an upper bound). We can convert each state to a unique integer to index the qtable. Indexing (calling in this case) the QTable with a state results in a row of the QTable containing 9 qvalues corresponding to the 9 possible actions one can take for each state. Yes, there are 9 possible actions possible at **each** state, even if there are already a few squares taken: the game is programmed in such a way that an invalid move will make you lose, obviously this design choice will make training more difficult but also more general. Note that we only use the *2D state* of the game as key for the Q-table as this is enough to create a unique key to index the underlying table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self):\n",
    "        self._qtable = np.random.randn(3 ** 9, 9)\n",
    "        self._state_key_vector = 3 ** np.arange(9)\n",
    "\n",
    "    def __call__(self, state2d):\n",
    "        return self._qtable[int(np.sum(state2d.ravel() * self._state_key_vector))]\n",
    "\n",
    "    def save(self, filename):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            filename = filename + \".csv\"\n",
    "        np.savetxt(filename, self._qtable, delimiter=\",\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            filename = filename + \".csv\"\n",
    "        self._qtable = np.loadtxt(filename, delimiter=\",\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "<img src=\"static/img/reinforcement-learning/robot.png\" width=200>\n",
    "\n",
    "A reinforcement agent plays the game by playing a random action with with probability ∆ê. Alternatively, the Agent will consult its Q-table with probability (1-∆ê) to play the action with the optimal Q-value associated to it. This is the *explore*-*exploit* principle. During training of the Agent the ∆ê will be quite high, as one want the agent to learn each corner case of the game. During evaluation, the ∆ê will be set to zero, forcing the agent to play its best game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" The Agent plays the game by playing a move corresponding to the optimal Q-value \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, qtable=None, epsilon=0.2, learning_rate=0.3, discount_factor=0.9\n",
    "    ):\n",
    "        \"\"\" A new Agent can be given some optional parameters to tune how fast it learns\n",
    "        \n",
    "        Args:\n",
    "            qtable: QTable=None: the initial Q-table to start with. \n",
    "            epsilon: float=0.2: the chance the Agent will explore a random move\n",
    "                               (in stead of choosing the optimal choice according to the Q table)\n",
    "            learning_rate: float=0.3: the rate at which the Agent learns from its games\n",
    "            discount_factor: float=0.9: the rate at which the final reward gets discounted\n",
    "                                        for when rating previous moves.\n",
    "        \"\"\"\n",
    "        self.qtable = QTable() if qtable is None else qtable\n",
    "\n",
    "        # the speed at which the Qvalues get updated\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # the discount factor of future rewards\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # the chance of executing a random action\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def random_action(self):\n",
    "        \"\"\" get a random action \"\"\"\n",
    "        return int(np.random.randint(0, 9, 1))\n",
    "\n",
    "    def best_action(self, state):\n",
    "        \"\"\" get the best values according to the current Q table \"\"\"\n",
    "        state2d, turn = state\n",
    "        argminmax = {1: np.argmax, 2: np.argmin}[turn]\n",
    "        qvalues = self.qtable(state2d)\n",
    "        return argminmax(qvalues)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\" perform an action according to the state on the game board \"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Choose action (random with chance of epsilon; best action otherwise.)\n",
    "            action = self.random_action()\n",
    "        else:\n",
    "            # get qvalues for current state of the game\n",
    "            action = self.best_action(state)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward):\n",
    "        \"\"\" learn from the current state and action taken. \"\"\"\n",
    "        state2d, turn = state\n",
    "        next_state2d, next_turn = next_state\n",
    "        if next_turn == 0:  # game finished\n",
    "            expected_qvalue_for_action = reward\n",
    "        else:\n",
    "            next_qvalues = self.qtable(next_state2d)\n",
    "            minmax = {1: max, 2: min}[next_turn]\n",
    "            expected_qvalue_for_action = reward + (\n",
    "                self.discount_factor * minmax(next_qvalues)\n",
    "            )\n",
    "\n",
    "        # update qvalues:\n",
    "        qvalues = self.qtable(state2d)\n",
    "        qvalues[action] += self.learning_rate * (\n",
    "            expected_qvalue_for_action - qvalues[action]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "<img src=\"static/img/reinforcement-learning/train.jpg\" width=300>\n",
    "\n",
    "Now the `Agent` is defined, it's time to train it! Training is as simple as just playing a 'bunch' of games (about 10,000,000). The `Agent` will learn automatically from its `learn` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000000/10000000 [26:55<00:00, 6189.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "N = 10_000_000  # Number of training episodes\n",
    "qtable = QTable()  # share Q for faster training\n",
    "player = Agent(qtable=qtable, learning_rate=0.1, epsilon=0.6)\n",
    "game = TicTacToe(player, player)  # let player play against itself\n",
    "\n",
    "for _ in tqdm.trange(N):\n",
    "    game.play()\n",
    "\n",
    "qtable.save(\"qtable.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play another game!\n",
    "Let the trained agent play against itself one final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1's turn:\n",
      ". . .\n",
      ". O .\n",
      ". . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . .\n",
      ". O .\n",
      ". . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . .\n",
      "O O .\n",
      ". . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . .\n",
      "O O X\n",
      ". . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . O\n",
      "O O X\n",
      ". . .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X . O\n",
      "O O X\n",
      "X . .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X . O\n",
      "O O X\n",
      "X O .\n",
      "\n",
      "\n",
      "player 2's turn:\n",
      "X X O\n",
      "O O X\n",
      "X O .\n",
      "\n",
      "\n",
      "player 1's turn:\n",
      "X X O\n",
      "O O X\n",
      "X O O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player = Agent(epsilon=0.0)  # epsilon=0 -> no random guesses\n",
    "player.qtable.load(\"qtable.csv\")\n",
    "game = TicTacToe(player, player)  # let player play against itself\n",
    "\n",
    "# play\n",
    "game.play(visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the game ends in a tie. This is to be expected, as two perfect tic-tac-toe players playing against each other will always draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table for the trained Agent can be downloaded in csv format [here](https://blob.flaport.net/qtable.csv) (availability not guarenteed).\n",
    "\n",
    "**[Go to part 2 of the series](reinforcement-learning-part-2.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
